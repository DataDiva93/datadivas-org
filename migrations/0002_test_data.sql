-- Test data for Data Divas wins table
-- This creates sample wins to showcase the leaderboard and wins feed

INSERT INTO wins (id, handle, repo_url, demo_url, summary, broke_fix, validation, resume_bullets, points, featured, created_at) VALUES
  (
    'test-001',
    'DataDiva93',
    'https://github.com/DataDiva93/airflow-pipeline',
    'https://demo.datadivas.org/airflow',
    'Built production Airflow DAG for ETL pipeline processing 1M+ records daily from Postgres to Snowflake with Great Expectations quality gates',
    'Airflow scheduler kept failing due to zombie processes. Fixed by implementing proper task cleanup and health checks. Added monitoring with Grafana.',
    'Successfully processed 30-day backfill, all quality checks passed, pipeline runs daily at 2AM UTC with 99.9% success rate',
    '• Engineered ETL pipeline using Apache Airflow processing 1M+ daily records with 99.9% reliability\n• Implemented data quality validation with Great Expectations reducing data errors by 85%',
    25,
    1,
    datetime('now', '-5 days')
  ),
  (
    'test-002',
    'CodeWarrior',
    'https://github.com/codewarrior/dbt-transform',
    NULL,
    'Migrated legacy SQL transforms to dbt with 50+ models, documentation, and automated testing',
    'dbt tests were taking 2+ hours to run. Optimized by creating test macros and parallelizing test execution. Reduced runtime to 15 minutes.',
    'All 50 models tested and documented, integrated with CI/CD pipeline, deployed to production serving 20+ analysts',
    '• Migrated 50+ SQL transformations to dbt framework improving code maintainability and testing\n• Reduced data transformation time by 60% through query optimization and incremental models',
    20,
    1,
    datetime('now', '-4 days')
  ),
  (
    'test-003',
    'PipelinePro',
    'https://github.com/pipelinepro/streaming-kafka',
    'https://demo.pipelinepro.dev',
    'Real-time streaming data pipeline with Kafka, PySpark, and PostgreSQL processing IoT sensor data',
    'Kafka consumer lag kept growing. Issue was inefficient batch processing. Implemented micro-batching with Spark Structured Streaming and tuned partition count.',
    'Processing 10K events/sec with <100ms latency, zero data loss during 72-hour stress test',
    '• Built real-time streaming pipeline with Kafka and PySpark processing 10K events/second\n• Achieved <100ms latency for IoT data ingestion and processing',
    20,
    0,
    datetime('now', '-3 days')
  ),
  (
    'test-004',
    'SQLQueen',
    'https://github.com/sqlqueen/postgres-warehouse',
    NULL,
    'Designed dimensional data warehouse with 15 fact tables and 30 dimension tables for e-commerce analytics',
    'Query performance was terrible on fact tables. Added composite indexes, partitioning by date, and implemented materialized views for common aggregations.',
    'Dashboard queries went from 45 seconds to under 2 seconds. Successfully serving 50+ concurrent users.',
    '• Designed star schema data warehouse with 15 fact and 30 dimension tables supporting analytics\n• Optimized query performance achieving 95% improvement through indexing and partitioning',
    15,
    0,
    datetime('now', '-2 days')
  ),
  (
    'test-005',
    'DockerNinja',
    'https://github.com/dockerninja/warehouse-in-a-box',
    'https://dockerninja.dev/demo',
    'Containerized local data warehouse with Postgres, dbt, Metabase, and Airflow - complete dev environment in Docker Compose',
    'Containers kept running out of memory. Set proper resource limits, optimized Postgres config, and added volume persistence for data.',
    'One-command setup working on Mac, Linux, and Windows. 10+ team members using for local development.',
    '• Created containerized data stack with Docker Compose enabling one-command environment setup\n• Reduced local development setup time from 3 hours to 5 minutes',
    15,
    0,
    datetime('now', '-1 day')
  ),
  (
    'test-006',
    'DataViz',
    'https://github.com/dataviz/tableau-pipeline',
    'https://public.tableau.com/profile/dataviz',
    'Built automated data pipeline feeding Tableau dashboards with dbt transformations and Prefect orchestration',
    'Tableau extracts were stale by several hours. Implemented webhook triggers from dbt to refresh Tableau extracts automatically after successful runs.',
    'Dashboards now update within 10 minutes of source data changes. 25+ stakeholders using daily.',
    '• Automated BI pipeline with dbt and Prefect reducing dashboard refresh time by 80%\n• Built 15+ Tableau dashboards serving 25+ stakeholders with near real-time data',
    15,
    0,
    datetime('now', '-12 hours')
  ),
  (
    'test-007',
    'SparkMaster',
    'https://github.com/sparkmaster/pyspark-etl',
    NULL,
    'Scaled ETL job from single-node Python to distributed PySpark processing 50GB+ daily datasets',
    'Initial PySpark job was slower than Python! Issue was too many small partitions. Repartitioned data properly and tuned executor memory.',
    'Processing time reduced from 6 hours to 45 minutes. Handles 3x data growth without changes.',
    '• Migrated ETL pipeline to PySpark achieving 8x performance improvement on large datasets\n• Optimized Spark job configuration reducing cost by 60% through efficient resource utilization',
    20,
    0,
    datetime('now', '-6 hours')
  ),
  (
    'test-008',
    'TechDiva',
    'https://github.com/techdiva/great-expectations-suite',
    NULL,
    'Implemented comprehensive data quality framework with Great Expectations covering 200+ validation rules',
    'Tests were too strict and failing on edge cases. Worked with stakeholders to define acceptable ranges and added business rule validations.',
    'Caught 15 data quality issues in first month. Zero bad data incidents since implementation.',
    '• Established data quality framework with Great Expectations implementing 200+ validation rules\n• Prevented data quality incidents reducing downstream errors by 100%',
    15,
    0,
    datetime('now', '-3 hours')
  ),
  (
    'test-009',
    'CloudArchitect',
    'https://github.com/cloudarchitect/terraform-data-stack',
    NULL,
    'Infrastructure as Code for complete data platform on AWS: S3, Glue, Redshift, Airflow using Terraform',
    'Terraform state conflicts when multiple people ran deployments. Implemented remote state with S3 + DynamoDB locking.',
    'Deployed to 3 environments (dev, staging, prod). Infrastructure changes now tracked in Git with proper reviews.',
    '• Automated cloud infrastructure deployment using Terraform reducing setup time from days to hours\n• Implemented IaC best practices with remote state management and modular design',
    20,
    0,
    datetime('now', '-2 hours')
  ),
  (
    'test-010',
    'APIBuilder',
    'https://github.com/apibuilder/fastapi-data-api',
    'https://api.datadivas.dev',
    'Built FastAPI service exposing data warehouse through REST API with authentication, rate limiting, and caching',
    'API was getting hammered and slowing down. Added Redis caching for expensive queries and implemented rate limiting with API keys.',
    'Serving 1000+ requests/day with 50ms average response time. 10+ applications consuming the API.',
    '• Developed REST API with FastAPI serving data to 10+ applications with sub-100ms response times\n• Implemented caching and rate limiting reducing database load by 75%',
    20,
    0,
    datetime('now', '-1 hour')
  );
